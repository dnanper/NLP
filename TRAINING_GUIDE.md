# ğŸš€ HÆ°á»›ng Dáº«n Training vÃ  Evaluation

## ğŸ“Š Dataset Splits

Dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh 3 táº­p **KHÃ”NG CHá»’NG CHÃ‰O** (no data leakage):

```
data/processed/
â”œâ”€â”€ train_tokenized.pkl       # Training set
â”œâ”€â”€ validation_tokenized.pkl  # Validation set (dÃ¹ng trong training)
â””â”€â”€ test_tokenized.pkl         # Test set (chá»‰ dÃ¹ng cuá»‘i cÃ¹ng)
```

**âœ… ÄÃºng:**

- `train` â†’ Training
- `validation` â†’ Validation trong quÃ¡ trÃ¬nh train
- `test` â†’ Evaluation cuá»‘i cÃ¹ng (KHÃ”NG dÃ¹ng trong training)

**âŒ Sai:**

- ~~Random split tá»« train thÃ nh train/val~~ (Ä‘Ã£ fix)
- ~~DÃ¹ng test trong training~~ (khÃ´ng bao giá» lÃ m)

## ğŸ¯ Training Workflow

### 1. Train Model (Vietnamese â†’ English)

```bash
python trainer/train.py
```

**QuÃ¡ trÃ¬nh:**

1. Load `train_tokenized.pkl` â†’ Training
2. Load `validation_tokenized.pkl` â†’ Validation (tÃ­nh val_loss má»—i epoch)
3. Save best model khi val_loss tháº¥p nháº¥t
4. **KHÃ”NG** sá»­ dá»¥ng test set

**Output:**

- `checkpoints/best_model_vi2en/best_model.pt` - Model tá»‘t nháº¥t
- `checkpoints/best_model_vi2en/latest.pt` - Checkpoint cuá»‘i cÃ¹ng
- `checkpoints/best_model_vi2en/training_curves.png` - Biá»ƒu Ä‘á»“ loss

### 2. Train Bidirectional (Vietnamese â†” English)

```bash
python trainer/train_bidirectional.py
```

**Äáº·c biá»‡t:**

- Má»—i cáº·p (vi, en) Ä‘Æ°á»£c dÃ¹ng 2 láº§n:
  - Láº§n 1: vi â†’ en
  - Láº§n 2: en â†’ vi
- Single model cho cáº£ 2 hÆ°á»›ng

**Output:**

- `checkpoints/best_model_bidirectional/best_model.pt`

### 3. Evaluate on Test Set (SAU KHI TRAIN XONG)

```bash
python trainer/evaluate.py
```

**QuÃ¡ trÃ¬nh:**

1. Load trained model tá»« checkpoint
2. Load `test_tokenized.pkl` (láº§n Ä‘áº§u tiÃªn dÃ¹ng)
3. Translate toÃ n bá»™ test set vá»›i beam search
4. TÃ­nh BLEU score

**Output:**

- In ra BLEU-1, BLEU-2, BLEU-3, BLEU-4
- `checkpoints/best_model_vi2en/test_results.txt`
- Show 5 vÃ­ dá»¥ translations

## ğŸ”§ Thay Äá»•i KÃ­ch Cá»¡ Model

Trong `trainer/train.py` hoáº·c `trainer/train_bidirectional.py`, tÃ¬m dÃ²ng:

```python
# Model configuration - THAY Äá»”I KÃCH Cá»  MODEL Táº I ÄÃ‚Y:
# .small() - 256d, 4 layers, ~60M params (fast training)
# .base()  - 512d, 6 layers, ~65M params (balanced) â­ RECOMMENDED
# .large() - 1024d, 6 layers, ~213M params (best quality)
# .deep()  - 512d, 12 layers (very deep)
config = TransformerConfig.base()  # <-- THAY Äá»”I á» ÄÃ‚Y
```

**Chá»n size phÃ¹ há»£p:**

| Size       | d_model | layers | params | Training Time | BLEU  | Recommend for             |
| ---------- | ------- | ------ | ------ | ------------- | ----- | ------------------------- |
| `.small()` | 256     | 4      | ~60M   | 6h            | 25-30 | Quick experiments, laptop |
| `.base()`  | 512     | 6      | ~65M   | 12h           | 30-35 | â­ Production, balanced   |
| `.large()` | 1024    | 6      | ~213M  | 24h           | 35-40 | Best quality, research    |
| `.deep()`  | 512     | 12     | ~120M  | 18h           | 32-37 | Very deep network         |

**VÃ­ dá»¥ thay Ä‘á»•i:**

```python
# For fast training (laptop, quick test)
config = TransformerConfig.small()

# For best quality (powerful GPU)
config = TransformerConfig.large()
```

## ğŸ“ˆ Monitoring Training

### During Training

**Training logs hiá»ƒn thá»‹:**

```
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1234/1234 [12:34<00:00, 1.64it/s, loss=3.4567, lr=0.000001]
Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [01:23<00:00, 1.48it/s, val_loss=3.2345]

============================================================
Epoch 1 Summary:
  Train Loss: 3.4567
  Val Loss:   3.2345
  LR:         0.000001
  Time:       756.2s
============================================================

âœ“ New best model saved! Val Loss: 3.2345
```

**Indicators:**

- âœ… **Val loss giáº£m** â†’ Model Ä‘ang há»c tá»‘t
- âš ï¸ **Val loss tÄƒng** â†’ Overfitting, cÃ¢n nháº¯c early stopping
- âœ… **Train loss > Val loss** â†’ CÃ²n room Ä‘á»ƒ train
- âš ï¸ **Train loss << Val loss** â†’ Overfitting

### Training Curves

Xem biá»ƒu Ä‘á»“: `checkpoints/best_model_*/training_curves.png`

**Dáº¥u hiá»‡u tá»‘t:**

- Loss giáº£m Ä‘á»u qua cÃ¡c epochs
- Val loss theo sÃ¡t train loss
- Learning rate decay smooth

**Dáº¥u hiá»‡u xáº¥u:**

- Val loss tÄƒng sá»›m â†’ Reduce learning rate
- Loss khÃ´ng giáº£m â†’ Check data/model config
- Val loss oscillate â†’ Reduce batch size

## ğŸ“ Best Practices

### 1. Start Small

```bash
# Thá»­ nghiá»‡m vá»›i small model trÆ°á»›c
config = TransformerConfig.small()
NUM_EPOCHS = 5
```

### 2. Monitor Validation

- Check val_loss má»—i epoch
- Save best model (tá»± Ä‘á»™ng)
- Early stopping náº¿u val_loss khÃ´ng giáº£m sau 3-5 epochs

### 3. Resume Training

```python
# Trong train.py main()
trainer.train(
    NUM_EPOCHS,
    save_every=1,
    resume_from='checkpoints/best_model_vi2en/latest.pt'
)
```

### 4. Test CUá»I CÃ™NG

- **KHÃ”NG** dÃ¹ng test set Ä‘á»ƒ tune hyperparameters
- **CHá»ˆ** evaluate trÃªn test set 1 láº§n cuá»‘i
- Use validation set Ä‘á»ƒ chá»n model

## ğŸ“ Example Workflow

```bash
# 1. Train model
python trainer/train.py
# â†’ Saves to checkpoints/best_model_vi2en/best_model.pt

# 2. Monitor training
# Watch training_curves.png
# Check val_loss in logs

# 3. If need to continue training
# Edit train.py: resume_from='checkpoints/.../latest.pt'
python trainer/train.py

# 4. FINAL evaluation on test set
python trainer/evaluate.py
# â†’ Prints BLEU scores
# â†’ Saves test_results.txt
# â†’ Shows sample translations

# 5. Use model for inference
from trainer import Translator
translator = Translator(
    checkpoint_path='checkpoints/best_model_vi2en/best_model.pt',
    vocab_path='SentencePiece-from-scratch/tokenizer_models/vocabulary.txt'
)
result = translator.translate("Xin chÃ o")
```

## ğŸ› Troubleshooting

### Training khÃ´ng giáº£m loss

- âœ… Check learning rate (máº·c Ä‘á»‹nh 1e-4 lÃ  tá»‘t)
- âœ… Verify data Ä‘Ãºng format
- âœ… Try smaller model (.small()) trÆ°á»›c

### Out of Memory (OOM)

- âœ… Giáº£m BATCH_SIZE trong `config.py`
- âœ… DÃ¹ng model nhá» hÆ¡n (.small())
- âœ… Giáº£m max_len

### Overfitting

- âœ… TÄƒng dropout (0.1 â†’ 0.3)
- âœ… TÄƒng label_smoothing (0.1 â†’ 0.15)
- âœ… Early stopping

### BLEU score tháº¥p

- âœ… Train longer (30+ epochs)
- âœ… DÃ¹ng model lá»›n hÆ¡n (.base() hoáº·c .large())
- âœ… Check data quality
- âœ… Use beam search trong inference (beam_size=4-6)

## ğŸ“š Summary

**Key Points:**

1. âœ… Train vá»›i train set + validation set
2. âœ… Validation trong má»—i epoch Ä‘á»ƒ chá»n best model
3. âœ… Test set CHá»ˆ dÃ¹ng CUá»I CÃ™NG Ä‘á»ƒ evaluate
4. âœ… Thay Ä‘á»•i model size báº±ng `.small()`, `.base()`, `.large()`
5. âœ… Monitor val_loss Ä‘á»ƒ detect overfitting
6. âœ… Use beam search cho inference cháº¥t lÆ°á»£ng cao

**Files to Run:**

- `trainer/train.py` - Training
- `trainer/train_bidirectional.py` - Bidirectional training
- `test_model.py` - Final evaluation on test set

Good luck! ğŸš€

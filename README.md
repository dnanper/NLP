# Vietnamese-English Translator

Complete Neural Machine Translation system using Transformer architecture.

## ğŸ“ Project Structure

```
ViEn_Translator/
â”‚
â”œâ”€â”€ models_best/              # ğŸ—ï¸ MODEL ARCHITECTURE ONLY
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ config.py            # TransformerConfig
â”‚   â”œâ”€â”€ transformer.py       # BestTransformer (main model)
â”‚   â”œâ”€â”€ encoder.py           # Pre-LN encoder
â”‚   â”œâ”€â”€ decoder.py           # Pre-LN decoder
â”‚   â”œâ”€â”€ attention.py         # Multi-head attention
â”‚   â”œâ”€â”€ feed_forward.py      # Feed-forward network
â”‚   â”œâ”€â”€ embeddings.py        # Embedding layers
â”‚   â”œâ”€â”€ positional_encoding.py  # Position encoding
â”‚   â”œâ”€â”€ layer_norm.py        # LayerNorm & RMSNorm
â”‚   â”œâ”€â”€ beam_search.py       # Beam search decoder
â”‚   â””â”€â”€ label_smoothing.py   # Label smoothing loss
â”‚
â”œâ”€â”€ trainer/                  # ğŸš€ TRAINING & INFERENCE
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py             # Training script (viâ†’en)
â”‚   â”œâ”€â”€ train_bidirectional.py  # Bidirectional (viâ†”en)
â”‚   â”œâ”€â”€ inference.py         # Translation inference
â”‚   â””â”€â”€ evaluate.py          # BLEU evaluation
â”‚
â”œâ”€â”€ utils/                    # ğŸ› ï¸ DATA PROCESSING
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_processing.py   # DataProcessor, Dataset, collate_fn
â”‚
â”œâ”€â”€ SentencePiece-from-scratch/  # ğŸ“ TOKENIZER
â”‚   â”œâ”€â”€ tokenizer_models/
â”‚   â”‚   â”œâ”€â”€ vocabulary.txt   # 32k vocab
â”‚   â”‚   â””â”€â”€ metadata.txt
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                     # ğŸ“Š DATASETS
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train_tokenized.pkl
â”‚       â”œâ”€â”€ validation_tokenized.pkl
â”‚       â””â”€â”€ test_tokenized.pkl
â”‚
â”œâ”€â”€ checkpoints/              # ğŸ’¾ SAVED MODELS
â”‚   â”œâ”€â”€ best_model_vi2en/
â”‚   â””â”€â”€ best_model_bidirectional/
â”‚
â”œâ”€â”€ config.py                 # Global config
â””â”€â”€ README.md                 # This file
```

## âœ¨ Features

### Model Architecture

- âœ… **Pre-Layer Normalization** - More stable training
- âœ… **Weight Tying** - Decoder embedding = output projection
- âœ… **Label Smoothing** - Better generalization (0.1)
- âœ… **Beam Search** - High-quality inference with length penalty
- âœ… **Multi-Query Attention** - Faster inference (optional)
- âœ… **Gradient Clipping** - Prevent gradient explosion

### Training Features

- âœ… **Warmup LR Scheduler** - Linear warmup + inverse sqrt decay
- âœ… **Mixed Precision Training** - Faster with modern GPUs
- âœ… **Checkpoint Management** - Auto-save best model
- âœ… **Training Curves** - Automatic plotting
- âœ… **Resume Training** - Load from checkpoint

### Data Processing

- âœ… **SentencePiece Tokenizer** - 32,000 BPE tokens
- âœ… **Cached Tokenization** - Fast data loading (.pkl files)
- âœ… **Proper Masking** - Padding mask + causal mask
- âœ… **Bidirectional Support** - Train single model for both directions

## ğŸš€ Quick Start

### 1. Train Vietnamese â†’ English

```bash
python trainer/train.py
```

**Configuration:**

- Model: Base (512d, 6 layers, 65M params)
- Batch size: From `config.Config.BATCH_SIZE`
- Max length: From `config.Config.MAX_LEN`
- Device: Auto-detect CUDA/CPU
- Saves to: `checkpoints/best_model_vi2en/`

### 2. Train Bidirectional (Vietnamese â†” English)

```bash
python trainer/train_bidirectional.py
```

**Key advantage:** Single model handles BOTH directions by using each (vi, en) pair twice:

- First time: vi â†’ en
- Second time: en â†’ vi

**Saves to:** `checkpoints/best_model_bidirectional/`

### 3. Inference (Translation)

```python
from trainer import Translator

translator = Translator(
    checkpoint_path='checkpoints/best_model_vi2en/best_model.pt',
    vocab_path='SentencePiece-from-scratch/tokenizer_models/vocabulary.txt'
)

# Translate a sentence
result = translator.translate("Xin chÃ o tháº¿ giá»›i")
print(result)  # "Hello world"

# Translate with beam search
result = translator.translate("TÃ´i yÃªu há»c mÃ¡y", beam_size=5)
print(result)
```

### 4. Evaluate BLEU Score

```python
from trainer import Evaluator

evaluator = Evaluator(
    checkpoint_path='checkpoints/best_model_vi2en/best_model.pt',
    vocab_path='SentencePiece-from-scratch/tokenizer_models/vocabulary.txt'
)

# Evaluate on test set
bleu_score = evaluator.evaluate_file(
    src_file='data/processed/test.vi',
    tgt_file='data/processed/test.en'
)
print(f"BLEU: {bleu_score:.2f}")
```

## ğŸ“Š Model Configurations

### Small (Fast Training)

```python
config = TransformerConfig.small()
# 256d, 4 layers, ~60M params
# Good for: Quick experiments, limited GPU
```

### Base (Recommended)

```python
config = TransformerConfig.base()
# 512d, 6 layers, ~65M params
# Good for: Production, balanced quality/speed
```

### Large (Best Quality)

```python
config = TransformerConfig.large()
# 1024d, 6 layers, ~213M params
# Good for: Maximum quality, research
```

### Deep (Very Deep Network)

```python
config = TransformerConfig.deep()
# 512d, 12 layers
# Good for: Complex language pairs
```

## ğŸ”§ Advanced Usage

### Resume Training

```python
# In train.py main() function
trainer.train(
    NUM_EPOCHS,
    save_every=1,
    resume_from='checkpoints/best_model_vi2en/latest.pt'
)
```

### Custom Configuration

```python
from models_best import TransformerConfig

config = TransformerConfig(
    d_model=512,
    n_encoder_layers=6,
    n_decoder_layers=6,
    n_heads=8,
    d_ff=2048,
    dropout=0.1,
    max_len=512,
    learning_rate=1e-4,
    warmup_steps=8000,
    label_smoothing=0.1
)
```

### Export for Production

```python
# Export model to TorchScript
model = BestTransformer(...)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

scripted_model = torch.jit.script(model)
scripted_model.save('model_production.pt')
```

## ğŸ“ˆ Training Tips

1. **Start with small model** to verify pipeline works
2. **Monitor validation loss** - stop if overfitting
3. **Use beam search** for inference (beam_size=4-6)
4. **Bidirectional training** gives better results with same data
5. **Gradient clipping** at 1.0 prevents instability
6. **Label smoothing** 0.1 is optimal for most cases

## ğŸ¯ Expected Results

| Model | BLEU (viâ†’en) | BLEU (enâ†’vi) | Training Time |
| ----- | ------------ | ------------ | ------------- |
| Small | ~25-30       | ~23-28       | ~6 hours      |
| Base  | ~30-35       | ~28-33       | ~12 hours     |
| Large | ~35-40       | ~33-38       | ~24 hours     |

_On single GPU (RTX 3090), ~1.3M training pairs_

## ğŸ“ Data Format

### Input Files

- `data/processed/train_tokenized.pkl` - Training data
- `data/processed/validation_tokenized.pkl` - Validation data
- `data/processed/test_tokenized.pkl` - Test data

### Format (Pickle)

```python
{
    'en': List[List[int]],  # English token IDs
    'vi': List[List[int]]   # Vietnamese token IDs
}
```

### Special Tokens

- PAD: 0
- UNK: 1
- SOS: 2 (Start of Sequence)
- EOS: 3 (End of Sequence)

## ğŸ› ï¸ Dependencies

```bash
pip install torch torchvision torchaudio
pip install sentencepiece
pip install tqdm matplotlib
```

## ğŸ“š References

- **Attention Is All You Need** - Vaswani et al. (2017)
- **Pre-LN Transformer** - Xiong et al. (2020)
- **Label Smoothing** - Szegedy et al. (2016)
- **SentencePiece** - Kudo & Richardson (2018)

## ğŸ¤ Contributing

Feel free to:

- Report bugs
- Suggest features
- Submit pull requests
- Improve documentation

## ğŸ“„ License

MIT License - See LICENSE file for details

---

**Happy Translating! ğŸŒâœ¨**

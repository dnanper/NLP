# ğŸ“Š LOSS CALCULATION - CHI TIáº¾T VÃ€ MASKING

## âœ… TL;DR: ÄÃƒ MASK ÄÃšNG!

**Loss chá»‰ tÃ­nh trÃªn cÃ¡c token KHÃ”NG pháº£i padding (PAD=0)**

---

## ğŸ” FLOW CHI TIáº¾T

### 1ï¸âƒ£ Khá»Ÿi Táº¡o Loss Function

```python
# In trainer/train.py - Trainer.__init__()
self.criterion = LabelSmoothingLoss(
    num_classes=model.tgt_vocab_size,      # 32000
    smoothing=config.label_smoothing,       # 0.1
    ignore_index=config.pad_idx             # 0 (PAD token)
)
```

**Key point:** `ignore_index=0` â†’ Loss sáº½ IGNORE táº¥t cáº£ PAD tokens!

---

### 2ï¸âƒ£ Training Step

```python
# In train_epoch()

# Original target sequence
tgt = [SOS, tok1, tok2, tok3, PAD, PAD]  # [B, T]

# Prepare input/output
tgt_input  = tgt[:, :-1]  # [SOS, tok1, tok2, tok3, PAD]     (input to decoder)
tgt_output = tgt[:, 1:]   # [tok1, tok2, tok3, PAD, PAD]     (target for loss)

# Forward pass
logits = model(src, tgt_input, src_mask, tgt_mask)
# logits: [B, T-1, vocab_size] = [B, 5, 32000]

# Compute loss (ÄÃ‚Y LÃ€ CHá»– MASK!)
loss = self.criterion(logits, tgt_output)
```

---

### 3ï¸âƒ£ Label Smoothing Loss - Masking Logic

```python
# In models_best/label_smoothing.py

def forward(self, logits, targets):
    # Flatten
    logits: [B, T, vocab_size] â†’ [B*T, vocab_size]
    targets: [B, T] â†’ [B*T]

    # BÆ¯á»šC 1: Táº O MASK CHO PADDING
    if self.ignore_index >= 0:  # ignore_index = 0 (PAD)
        mask = targets.ne(self.ignore_index)  # âœ… mask[i] = True náº¿u targets[i] != 0

    # VÃ­ dá»¥:
    # targets = [123, 456, 789, 0, 0]  # 3 real tokens, 2 PAD
    # mask    = [True, True, True, False, False]  # âœ… ÄÃšNG!

    # BÆ¯á»šC 2: Táº O SMOOTH TARGETS (CHá»ˆ CHO NON-PAD TOKENS)
    with torch.no_grad():
        smooth_targets = torch.full_like(log_probs, self.smoothing_value)
        smooth_targets.scatter_(1, targets.unsqueeze(1), self.confidence)

        # ZERO OUT PADDING TOKENS
        smooth_targets = smooth_targets * mask.unsqueeze(1).float()
        # âœ… PAD positions â†’ all zeros in smooth_targets

    # BÆ¯á»šC 3: COMPUTE LOSS
    loss = -torch.sum(smooth_targets * log_probs, dim=-1)
    # loss: [B*T] - loss per position

    # BÆ¯á»šC 4: MASK LOSS (CHá»ˆ GIá»® NON-PAD)
    loss = loss * mask  # âœ… PAD positions â†’ loss = 0

    # BÆ¯á»šC 5: AVERAGE CHá»ˆ TRÃŠN VALID TOKENS
    return loss.sum() / mask.sum()  # âœ… Divide by NUMBER OF NON-PAD TOKENS
```

---

## ğŸ“Š VÃ Dá»¤ Cá»¤ THá»‚

### Input Batch:

```python
# Batch size = 2
tgt_output = [
    [tok1, tok2, tok3, PAD, PAD],  # Sentence 1: 3 real tokens
    [tok4, tok5, PAD, PAD, PAD],   # Sentence 2: 2 real tokens
]

# Total positions: 2 * 5 = 10
# Valid tokens: 3 + 2 = 5
# PAD tokens: 5 (WILL BE IGNORED)
```

### Loss Computation:

```python
# After model forward
logits: [2, 5, 32000]
targets: [2, 5]

# Flatten
logits: [10, 32000]
targets: [10] = [tok1, tok2, tok3, 0, 0, tok4, tok5, 0, 0, 0]

# Create mask
mask = [True, True, True, False, False, True, True, False, False, False]

# Compute per-position loss
loss_per_pos = [L1, L2, L3, L4, L5, L6, L7, L8, L9, L10]

# Apply mask
loss_per_pos = [L1, L2, L3, 0, 0, L6, L7, 0, 0, 0]
                  âœ…  âœ…  âœ…  âŒ  âŒ  âœ…  âœ…  âŒ  âŒ  âŒ

# Final loss
total_loss = L1 + L2 + L3 + L6 + L7
avg_loss = total_loss / 5  # Divide by 5 NON-PAD tokens, NOT 10!
```

---

## âœ… KIá»‚M TRA: MASK ÄÃšNG CHÆ¯A?

### Checkpoint 1: ignore_index

```python
âœ… self.criterion = LabelSmoothingLoss(ignore_index=0)
```

### Checkpoint 2: Mask creation

```python
âœ… mask = targets.ne(self.ignore_index)  # True for non-PAD
```

### Checkpoint 3: Zero out smooth_targets for PAD

```python
âœ… smooth_targets = smooth_targets * mask.unsqueeze(1).float()
```

### Checkpoint 4: Mask loss values

```python
âœ… loss = loss * mask  # PAD positions â†’ 0
```

### Checkpoint 5: Average over valid tokens only

```python
âœ… return loss.sum() / mask.sum()  # Divide by NON-PAD count
```

---

## ğŸ¯ Káº¾T LUáº¬N

### âœ… ÄÃšNG - Loss Ä‘Ã£ Ä‘Æ°á»£c mask HOÃ€N TOÃ€N!

**CÃ¡c bÆ°á»›c masking:**

1. âœ… Táº¡o mask: `targets != PAD_IDX`
2. âœ… Zero out smooth_targets cho PAD positions
3. âœ… Zero out loss cho PAD positions
4. âœ… Average chá»‰ trÃªn valid tokens (khÃ´ng tÃ­nh PAD)

**Háº­u quáº£:**

- PAD tokens KHÃ”NG Ä‘Ã³ng gÃ³p vÃ o loss
- Gradient KHÃ”NG Ä‘Æ°á»£c tÃ­nh cho PAD positions
- Model KHÃ”NG há»c tá»« PAD tokens
- Training chá»‰ focus vÃ o real tokens âœ…

---

## ğŸ”¬ SO SÃNH Vá»šI STANDARD CROSS ENTROPY

### Standard CrossEntropyLoss:

```python
# PyTorch's built-in
criterion = nn.CrossEntropyLoss(ignore_index=0)
loss = criterion(logits, targets)
# âœ… CÅ©ng mask PAD, nhÆ°ng KHÃ”NG cÃ³ label smoothing
```

### Our LabelSmoothingLoss:

```python
criterion = LabelSmoothingLoss(ignore_index=0, smoothing=0.1)
loss = criterion(logits, targets)
# âœ… Mask PAD + Label Smoothing (better generalization)
```

---

## ğŸ“ˆ IMPACT

### Náº¿u KHÃ”NG mask PAD:

```python
âŒ loss = loss.mean()  # Divide by ALL positions
# â†’ Loss sáº½ Bá»Š GIáº¢M GIáº¢M vÃ¬ PAD chiáº¿m nhiá»u
# â†’ Model há»c sai: predict PAD quÃ¡ nhiá»u
# â†’ BLEU score tháº¥p
```

### Vá»›i mask PAD (hiá»‡n táº¡i):

```python
âœ… loss = loss.sum() / mask.sum()  # Divide by VALID tokens
# â†’ Loss pháº£n Ã¡nh Ä‘Ãºng performance trÃªn real tokens
# â†’ Model há»c Ä‘Ãºng distribution
# â†’ BLEU score cao hÆ¡n
```

---

## ğŸ§ª VERIFY CODE

Äá»ƒ kiá»ƒm tra mask hoáº¡t Ä‘á»™ng Ä‘Ãºng, add test case:

```python
def test_loss_masking():
    """Test that PAD tokens are ignored in loss"""
    vocab_size = 100
    criterion = LabelSmoothingLoss(
        num_classes=vocab_size,
        smoothing=0.1,
        ignore_index=0
    )

    # Create fake logits and targets
    logits = torch.randn(4, 10, vocab_size)  # [B=4, T=10, V=100]
    targets = torch.randint(1, 100, (4, 10))  # [B=4, T=10]

    # Add PAD tokens
    targets[:, 7:] = 0  # Last 3 positions are PAD

    # Compute loss
    loss = criterion(logits, targets)

    # Verify: should only compute loss on first 7 positions
    # If PAD is NOT masked, loss would be much smaller

    print(f"Loss with mask: {loss.item():.4f}")

    # Test: Set PAD positions to extreme values
    targets_test = targets.clone()
    targets_test[:, 7:] = 99  # Change PAD to valid token

    loss_no_pad = criterion(logits, targets_test)
    print(f"Loss without PAD: {loss_no_pad.item():.4f}")

    # Loss should be different if mask works
    assert abs(loss.item() - loss_no_pad.item()) > 0.01
    print("âœ… Masking works correctly!")
```

---

## ğŸ“ SUMMARY

**Question:** LÃºc train, tÃ­nh loss nhÆ° tháº¿ nÃ o? ÄÃ£ mask cÃ¡c token cáº§n tÃ­nh loss chÆ°a?

**Answer:**
âœ… **ÄÃƒ MASK ÄÃšNG!**

- Loss function: `LabelSmoothingLoss(ignore_index=0)`
- PAD tokens (0) Ä‘Æ°á»£c IGNORE hoÃ n toÃ n
- Loss chá»‰ tÃ­nh trÃªn VALID tokens
- Average loss = total_loss / sá»‘_valid_tokens

**Implementation:** CHUáº¨N âœ…
